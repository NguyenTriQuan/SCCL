{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([51.6803, 46.6756])\n",
      "tensor([8, 1, 0, 7, 3, 2, 4, 5, 6, 9])\n",
      "tensor([2, 8, 1, 7, 6, 3, 0, 4, 5, 9])\n",
      "tensor([6, 2, 5, 8, 9, 1, 0, 3, 4, 7])\n",
      "tensor([55.1321, 51.8804])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(10, 3)\n",
    "v, i  = a.norm(2, dim=1).sort(descending=True)\n",
    "a[i[5:]] = 0\n",
    "b = torch.rand(10, 10)\n",
    "b[:, i[5:]] = 0\n",
    "v, i  = b.norm(2, dim=1).sort(descending=True)\n",
    "b[i[6:]] = 0\n",
    "c = torch.rand(10, 10)\n",
    "c[:, i[6:]] = 0\n",
    "v, i  = c.norm(2, dim=1).sort(descending=True)\n",
    "c[i[7:]] = 0\n",
    "d = torch.rand(2, 10)\n",
    "d[:, i[7:]] = 0\n",
    "\n",
    "x = torch.rand(3)\n",
    "y = torch.matmul(a, x)\n",
    "y = torch.matmul(b, y) + y\n",
    "y = torch.matmul(c, y) + y\n",
    "y = torch.matmul(d, y)\n",
    "print(y)\n",
    "\n",
    "v, i  = a.norm(2, dim=1).sort(descending=True)\n",
    "print(i)\n",
    "a = a[i[:5]]\n",
    "b = b[:, i[:5]]\n",
    "\n",
    "v, i  = b.norm(2, dim=1).sort(descending=True)\n",
    "print(i)\n",
    "b = b[i[:6]]\n",
    "c = c[:, i[:6]]\n",
    "\n",
    "v, i  = c.norm(2, dim=1).sort(descending=True)\n",
    "print(i)\n",
    "c = c[i[:7]]\n",
    "d = d[:, i[:7]]\n",
    "\n",
    "y = torch.matmul(a, x)\n",
    "y_ = torch.matmul(b, y)\n",
    "y_[:5] += y\n",
    "y = y_\n",
    "\n",
    "y_ = torch.matmul(c, y)\n",
    "y_[:6] += y\n",
    "y = y_\n",
    "\n",
    "y = torch.matmul(d, y)\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Documents\\Python-projects\\SCCL_project\\SCCL\n",
      "s_H=1.4e+07\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\Admin\\Documents\\Python-projects\\SCCL_project\\SCCL\n",
    "name = 'split_cifar100_sccl_VGG8_0_lamb_0.015_lr_0.001_batch_32_epoch_100_optim_Adam_fix_False.model'\n",
    "dir = 'C:\\\\Users\\\\Admin\\\\Documents\\\\Python-projects\\\\SCCL_project\\\\result_data\\\\trained_model\\\\'\n",
    "\n",
    "model = torch.load(dir+name)['model']\n",
    "s_H = model.s_H()\n",
    "print('s_H={:.1e}'.format(s_H))\n",
    "t = 1\n",
    "# model.get_params(t-1)\n",
    "# for m in model.DM:\n",
    "#     weight = torch.cat([torch.cat([m.old_weight, m.fwt_weight[t]], dim=0), torch.cat([m.bwt_weight[t], m.weight[t]], dim=0)], dim=1)\n",
    "#     norm = weight.norm(2).detach()\n",
    "#     # norm = s_H\n",
    "#     m.weight[t].data /= norm\n",
    "#     if m.bias:\n",
    "#         m.bias[t].data /= norm\n",
    "    \n",
    "#     if m.norm_layer:\n",
    "#         if m.norm_layer.track_running_stats:\n",
    "#             m.norm_layer.running_mean[t].data /= norm\n",
    "#         if m.norm_layer.affine:\n",
    "#             m.norm_layer.weight[t].data /= norm\n",
    "#             m.norm_layer.bias[t].data /= norm\n",
    "# s_H = model.s_H()\n",
    "# print('s_H={:.1e}'.format(s_H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Appr(object):\n",
    "\n",
    "    def __init__(self,model,args=None,thres=1e-3,lamb='0',nepochs=100,batch_size=256,val_batch_size=256,\n",
    "                lr=0.001,lr_min=1e-5,lr_factor=3,lr_patience=5,clipgrad=10,optim='Adam',tasknum=1,fix=False, experiment=None, approach=None, arch=None, seed=0):\n",
    "        self.model=model\n",
    "\n",
    "        self.nepochs = nepochs\n",
    "        self.batch_size = batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.lr = lr\n",
    "        self.lr_min = lr/100\n",
    "        self.lr_factor = lr_factor\n",
    "        self.lr_patience = lr_patience \n",
    "        self.clipgrad = clipgrad\n",
    "        self.optim = optim\n",
    "        self.thres = thres\n",
    "        self.tasknum = tasknum\n",
    "        self.fix = fix\n",
    "        self.experiment = experiment\n",
    "        self.approach = approach\n",
    "        self.arch = arch\n",
    "        self.seed = seed\n",
    "\n",
    "        self.lambs = [float(i) for i in lamb.split('_')]\n",
    "        self.check_point = None\n",
    "        \n",
    "        if len(self.lambs) < self.tasknum:\n",
    "            self.lambs = [self.lambs[-1] if i>=len(self.lambs) else self.lambs[i] for i in range(args.tasknum)]\n",
    "\n",
    "        print('lambs:', self.lambs)\n",
    "\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = self._get_optimizer()\n",
    "        self.shape_out = self.model.layers[-1].shape_out\n",
    "        self.cur_task = len(self.shape_out)-1\n",
    "\n",
    "        \n",
    "    def resume(self):\n",
    "        for t in range(1, self.tasknum + 1):\n",
    "            try:\n",
    "                self.log_name = '{}_{}_{}_{}_lamb_{}_lr_{}_batch_{}_epoch_{}_optim_{}_fix_{}'.format(self.experiment, self.approach, self.arch, self.seed,\n",
    "                                                                                '_'.join([str(lamb) for lamb in self.lambs[:t]]),  \n",
    "                                                                                self.lr, self.batch_size, self.nepochs, self.optim, self.fix)\n",
    "\n",
    "                self.check_point = torch.load(f'../result_data/trained_model/{self.log_name}.model')\n",
    "                self.model = self.check_point['model']\n",
    "                print('Resume from task', t-1)\n",
    "\n",
    "                return t-1\n",
    "            except:\n",
    "                continue\n",
    "        return 0\n",
    "\n",
    "    def _get_optimizer(self,lr=None):\n",
    "        if lr is None: lr=self.lr\n",
    "\n",
    "        params = self.model.get_optim_params()\n",
    "\n",
    "        if self.optim == 'SGD':\n",
    "            return torch.optim.SGD(params, lr=lr,\n",
    "                          weight_decay=0.0, momentum=0.9, nesterov=True)\n",
    "        if self.optim == 'Adam':\n",
    "            return torch.optim.Adam(params, lr=lr)\n",
    "\n",
    "    def train(self, t, train_loader, valid_loader, ncla=0):\n",
    "\n",
    "        if self.check_point is None:\n",
    "            print('Training new task')\n",
    "\n",
    "            self.model.expand(ncla)\n",
    "\n",
    "            self.shape_out = self.model.layers[-1].shape_out\n",
    "            self.cur_task = len(self.shape_out)-1\n",
    "\n",
    "            self.check_point = {'model':self.model, 'squeeze':True, 'optimizer':self._get_optimizer(), 'epoch':-1, 'lr':self.lr, 'patience':self.lr_patience}\n",
    "\n",
    "            try:\n",
    "                os.remove(f'../result_data/trained_model/{self.log_name}.model')\n",
    "            except:\n",
    "                pass\n",
    "            self.log_name = '{}_{}_{}_{}_lamb_{}_lr_{}_batch_{}_epoch_{}_optim_{}_fix_{}'.format(self.experiment, self.approach, self.arch, self.seed,\n",
    "                                                                                '_'.join([str(lamb) for lamb in self.lambs[:t]]),  \n",
    "                                                                                self.lr, self.batch_size, self.nepochs, self.optim, self.fix)\n",
    "            torch.save(self.check_point, f'../result_data/trained_model/{self.log_name}.model')\n",
    "                \n",
    "            with open(f'../result_data/csv_data/{self.log_name}.csv', 'w', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "                writer.writerow(['train loss', 'train acc', 'valid loss', 'valid acc', 'fro norm'])\n",
    "        else: \n",
    "            print('Retraining current task')\n",
    "\n",
    "\n",
    "        self.model.restrict_gradients(t-1, False)\n",
    "        self.shape_out = self.model.layers[-1].shape_out\n",
    "        self.cur_task = len(self.shape_out)-1\n",
    "\n",
    "        self.lamb = self.lambs[self.cur_task-1]\n",
    "        print('lambda', self.lamb)\n",
    "        print(self.log_name)\n",
    "\n",
    "        self.train_phase(t, train_loader, valid_loader, True)\n",
    "        if not self.check_point['squeeze']:\n",
    "            self.check_point = None\n",
    "            return \n",
    "\n",
    "        self.prune(t, train_loader, thres=self.thres)\n",
    "\n",
    "\n",
    "        self.check_point = {'model':self.model, 'squeeze':False, 'optimizer':self._get_optimizer(), 'epoch':-1, 'lr':self.lr, 'patience':self.lr_patience}\n",
    "        torch.save(self.check_point,'../result_data/trained_model/{}.model'.format(self.log_name))\n",
    "\n",
    "        self.train_phase(t, train_loader, valid_loader, False)\n",
    "\n",
    "        self.check_point = None\n",
    "        # self.model.get_params(t-1)\n",
    "        # for m in self.model.DM:\n",
    "        #     weight = torch.cat([torch.cat([m.old_weight, m.fwt_weight[t]], dim=0), torch.cat([m.bwt_weight[t], m.weight[t]], dim=0)], dim=1)\n",
    "        #     norm = weight.norm(2).detach()\n",
    "        #     m.weight[t].data /= norm\n",
    "        #     if m.bias:\n",
    "        #         m.bias[t].data /= norm\n",
    "\n",
    "        # s_H = self.model.s_H()\n",
    "        # print('s_H={:.1e}'.format(s_H), end='')\n",
    "        \n",
    "\n",
    "    def train_phase(self, t, train_loader, valid_loader, squeeze):\n",
    "\n",
    "        print('number of neurons:', end=' ')\n",
    "        for m in self.model.DM:\n",
    "            print(m.out_features, end=' ')\n",
    "        print()\n",
    "        params = self.model.compute_model_size()\n",
    "        print('num params', params)\n",
    "\n",
    "        train_loss,train_acc=self.eval(t,train_loader)\n",
    "        print('| Train: loss={:.3f}, acc={:5.2f}% |'.format(train_loss,100*train_acc), end='')\n",
    "\n",
    "        valid_loss,valid_acc=self.eval(t,valid_loader)\n",
    "        print(' Valid: loss={:.3f}, acc={:5.2f}% |'.format(valid_loss,100*valid_acc))\n",
    "\n",
    "        lr = self.check_point['lr']\n",
    "        patience = self.check_point['patience']\n",
    "        self.optimizer = self.check_point['optimizer']\n",
    "        start_epoch = self.check_point['epoch'] + 1\n",
    "        squeeze = self.check_point['squeeze']\n",
    "\n",
    "        if squeeze:\n",
    "            best_acc = train_acc\n",
    "        else:\n",
    "            best_acc = valid_acc\n",
    "    \n",
    "        try:\n",
    "            for e in range(start_epoch, self.nepochs):\n",
    "                clock0=time.time()\n",
    "                self.train_epoch(t, train_loader, squeeze)\n",
    "            \n",
    "                clock1=time.time()\n",
    "                train_loss,train_acc=self.eval(t, train_loader)\n",
    "                clock2=time.time()\n",
    "                print('| Epoch {:3d}, time={:5.1f}ms/{:5.1f}ms | Train: loss={:.3f}, acc={:5.2f}% |'.format(\n",
    "                    e+1,1000*(clock1-clock0),\n",
    "                    1000*(clock2-clock1),train_loss,100*train_acc),end='')\n",
    "\n",
    "                valid_loss,valid_acc=self.eval(t, valid_loader)\n",
    "                print(' Valid: loss={:.3f}, acc={:5.2f}% |'.format(valid_loss,100*valid_acc),end='')\n",
    "                \n",
    "                s_H = self.model.s_H()\n",
    "                print('s_H={:.1e}'.format(s_H), end='')\n",
    "                # Adapt lr\n",
    "                if squeeze:\n",
    "                    if train_acc >= best_acc:\n",
    "                        best_acc = train_acc\n",
    "                        self.check_point = {'model':self.model, 'optimizer':self.optimizer, 'squeeze':squeeze, 'epoch':e, 'lr':lr, 'patience':patience}\n",
    "                        torch.save(self.check_point,'../result_data/trained_model/{}.model'.format(self.log_name))\n",
    "                        print(' *', end='')\n",
    "                        patience = self.lr_patience\n",
    "\n",
    "                else:\n",
    "                    if valid_acc > best_acc:\n",
    "                        best_acc = valid_acc\n",
    "                        self.check_point = {'model':self.model, 'optimizer':self.optimizer, 'squeeze':squeeze, 'epoch':e, 'lr':lr, 'patience':patience}\n",
    "                        torch.save(self.check_point,'../result_data/trained_model/{}.model'.format(self.log_name))\n",
    "                        patience = self.lr_patience\n",
    "                        print(' *', end='')\n",
    "                    else:\n",
    "                        patience -= 1\n",
    "                        if patience <= 0:\n",
    "                            lr /= self.lr_factor\n",
    "                            print(' lr={:.1e}'.format(lr), end='')\n",
    "                            if lr < self.lr_min:\n",
    "                                print()\n",
    "                                break\n",
    "                                \n",
    "                            patience = self.lr_patience\n",
    "                            self.optimizer = self._get_optimizer(lr)\n",
    "\n",
    "                print()\n",
    "                with open(f'../result_data/csv_data/{self.log_name}.csv', 'a', newline='') as csvfile:\n",
    "                    writer = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "                    writer.writerow([train_loss, train_acc, valid_loss, valid_acc, s_H])\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print('KeyboardInterrupt')\n",
    "            self.check_point = torch.load('../result_data/trained_model/{}.model'.format(self.log_name))\n",
    "            self.model = self.check_point['model']\n",
    "            self.model.to(device)\n",
    "\n",
    "        self.check_point = torch.load('../result_data/trained_model/{}.model'.format(self.log_name))\n",
    "        self.model = self.check_point['model']\n",
    "\n",
    "    def train_batch(self, t, images, targets, squeeze):\n",
    "        outputs = self.model.forward(images, t=t)\n",
    "        outputs = outputs[:, self.shape_out[t-1]:self.shape_out[t]]\n",
    "\n",
    "        loss = self.ce(outputs, targets)\n",
    "\n",
    "        if squeeze:\n",
    "            loss += self.model.group_lasso_reg() * self.lamb\n",
    "                \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward() \n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def eval_batch(self, t, images, targets):\n",
    "        if t is None:\n",
    "            outputs = []\n",
    "            entropy = []\n",
    "            aug_images = [images]\n",
    "            batch_DA = 32\n",
    "            for n in range(batch_DA):\n",
    "                aug_images.append(self.trans(images))\n",
    "            aug_images = torch.cat(aug_images, dim=0)\n",
    "            for task in range(1, self.cur_task + 1):\n",
    "                self.model.get_params(task-1)\n",
    "                output = self.model.forward(aug_images, t=task)[:, self.shape_out[task-1]:self.shape_out[task]]\n",
    "                output = output.reshape(batch_DA+1, len(targets), -1)\n",
    "                # output = F.softmax(output, dim=-1)\n",
    "                output = F.softplus(output)\n",
    "                output = output / output.sum(-1).unsqueeze(-1)\n",
    "                outputs.append(output[0])\n",
    "                entropy.append(-(output*output.log()).sum((-1, 0)))\n",
    "\n",
    "            entropy = torch.stack(entropy, dim=1)\n",
    "            outputs = torch.stack(outputs, dim=1)\n",
    "            v, i = entropy.min(1)\n",
    "            outputs = outputs[range(outputs.shape[0]), i]\n",
    "        else:\n",
    "            self.model.get_params(t-1)\n",
    "            outputs = self.model.forward(images, t=t)\n",
    "            outputs = outputs[:, self.shape_out[t-1]:self.shape_out[t]]\n",
    "                        \n",
    "        loss=self.ce(outputs,targets)\n",
    "        values,indices=outputs.max(1)\n",
    "        hits=(indices==targets).float()\n",
    "\n",
    "        return loss.data.cpu().numpy()*len(targets), hits.sum().data.cpu().numpy()\n",
    "\n",
    "    def train_epoch(self, t, data_loader, squeeze=True):\n",
    "        self.model.train()\n",
    "        self.model.get_params(t-1)\n",
    "        for images, targets in data_loader:\n",
    "            images=images.to(device)\n",
    "            targets=targets.to(device)\n",
    "            self.train_batch(t, images, targets, squeeze)\n",
    "\n",
    "\n",
    "    def eval(self, t, data_loader):\n",
    "        total_loss=0\n",
    "        total_acc=0\n",
    "        total_num=0\n",
    "        self.model.eval()\n",
    "\n",
    "        for images, targets in data_loader:\n",
    "            images=images.to(device)\n",
    "            targets=targets.to(device)\n",
    "                    \n",
    "            loss, hits = self.eval_batch(t, images, targets)\n",
    "            total_loss += loss\n",
    "            total_acc += hits\n",
    "            total_num += len(targets)\n",
    "                \n",
    "        return total_loss/total_num,total_acc/total_num\n",
    "\n",
    "\n",
    "    def prune(self, t, data_loader, thres=0.0):\n",
    "\n",
    "        fig, axs = plt.subplots(3, len(self.model.DM)-1, figsize=(3*len(self.model.DM)-3, 9))\n",
    "        for i, m in enumerate(self.model.DM[:-1]):\n",
    "            axs[0][i].hist(m.norm_in().detach().cpu().numpy(), bins=100)\n",
    "            axs[0][i].set_title(f'layer {i+1}')\n",
    "\n",
    "            axs[1][i].hist(m.norm_out().detach().cpu().numpy(), bins=100)\n",
    "            axs[1][i].set_title(f'layer {i+1}')\n",
    "\n",
    "            axs[2][i].hist((m.norm_in()*m.norm_out()).detach().cpu().numpy(), bins=100)\n",
    "            axs[2][i].set_title(f'layer {i+1}')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        loss,acc=self.eval(t,data_loader)\n",
    "        loss, acc = round(loss, 3), round(acc, 3)\n",
    "        print('Pre Prune: loss={:.3f}, acc={:5.2f}% |'.format(loss,100*acc))\n",
    "        # pre_prune_acc = acc\n",
    "        pre_prune_loss = loss\n",
    "        prune_ratio = np.ones(len(self.model.DM)-1)\n",
    "        step = 0\n",
    "        pre_sum = 0\n",
    "        # Dynamic expansion\n",
    "        while True:\n",
    "            t1 = time.time()\n",
    "            fig, axs = plt.subplots(1, len(self.model.DM)-1, figsize=(3*len(self.model.DM)-3, 2))\n",
    "            print('Pruning ratio:', end=' ')\n",
    "            for i in range(0, len(self.model.DM)-1):\n",
    "                m = self.model.DM[i]\n",
    "                mask_temp = m.mask\n",
    "                norm = m.get_importance()\n",
    "\n",
    "                low = 0 \n",
    "                if m.mask is None:\n",
    "                    high = norm.shape[0]\n",
    "                else:\n",
    "                    high = int(sum(m.mask))\n",
    "\n",
    "                axs[i].hist(norm.detach().cpu().numpy(), bins=100)\n",
    "                axs[i].set_title(f'layer {i+1}')\n",
    "\n",
    "                if norm.shape[0] != 0:\n",
    "                    values, indices = norm.sort(descending=True)\n",
    "                    loss,acc=self.eval(t,data_loader)\n",
    "                    loss, acc = round(loss, 3), round(acc, 3)\n",
    "                    pre_prune_loss = loss\n",
    "\n",
    "                    while True:\n",
    "                        k = (high+low)//2\n",
    "                        # Select top-k biggest norm\n",
    "                        m.mask = (norm>values[k])\n",
    "                        loss, acc = self.eval(t, data_loader)\n",
    "                        loss, acc = round(loss, 3), round(acc, 3)\n",
    "                        # post_prune_acc = acc\n",
    "                        post_prune_loss = loss\n",
    "                        if  post_prune_loss <= pre_prune_loss:\n",
    "                        # if pre_prune_acc <= post_prune_acc:\n",
    "                            # k is satisfy, try smaller k\n",
    "                            high = k\n",
    "                            # pre_prune_loss = post_prune_loss\n",
    "                        else:\n",
    "                            # k is not satisfy, try bigger k\n",
    "                            low = k\n",
    "\n",
    "                        if k == (high+low)//2:\n",
    "                            break\n",
    "\n",
    "\n",
    "                if high == norm.shape[0]:\n",
    "                    # not found any k satisfy, keep all neurons\n",
    "                    m.mask = mask_temp\n",
    "                else:\n",
    "                    # found k = high is the smallest k satisfy\n",
    "                    m.mask = (norm>values[high])\n",
    "\n",
    "                # remove neurons \n",
    "                # m.squeeze()\n",
    "\n",
    "                if m.mask is None:\n",
    "                    prune_ratio[i] = 0.0\n",
    "                else:\n",
    "                    mask_count = int(sum(m.mask))\n",
    "                    total_count = m.mask.numel()\n",
    "                    prune_ratio[i] = 1.0 - mask_count/total_count\n",
    "\n",
    "                print('{:.3f}'.format(prune_ratio[i]), end=' ')\n",
    "                # m.mask = None\n",
    "\n",
    "            fig.savefig(f'../result_data/images/{self.log_name}_task{t}_step_{step}.pdf', bbox_inches='tight')\n",
    "            # plt.show()\n",
    "            loss,acc=self.eval(t,data_loader)\n",
    "            print('| Post Prune: loss={:.3f}, acc={:5.2f}% | Time={:5.1f}ms |'.format(loss, 100*acc, (time.time()-t1)*1000))\n",
    "\n",
    "            step += 1\n",
    "            if sum(prune_ratio) == pre_sum:\n",
    "                break\n",
    "            pre_sum = sum(prune_ratio)\n",
    "\n",
    "        for m in self.model.DM[:-1]:\n",
    "            m.squeeze()\n",
    "            m.mask = None\n",
    "        loss,acc=self.eval(t,data_loader)\n",
    "        print('Post Prune: loss={:.3f}, acc={:5.2f}% |'.format(loss,100*acc))\n",
    "\n",
    "        print('number of neurons:', end=' ')\n",
    "        for m in self.model.DM:\n",
    "            print(m.out_features, end=' ')\n",
    "        print()\n",
    "        params = self.model.compute_model_size()\n",
    "        print('num params', params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambs: [0.0]\n",
      "Task order = [ 3  9  5 10  2  7  8  4  1  6]\n",
      ">>> Test on task  0 - cifar100-3     : loss=0.652, acc=84.40% <<<\n"
     ]
    }
   ],
   "source": [
    "appr = Appr(model,args=None,thres=1e-3,lamb='0',nepochs=100,batch_size=256,val_batch_size=256,\n",
    "                lr=0.001,lr_min=1e-5,lr_factor=3,lr_patience=5,clipgrad=10,optim='Adam',tasknum=1,fix=False, experiment='split_cifar100', approach='sccl', arch='VGG8', seed=0)\n",
    "\n",
    "dataloader = importlib.import_module('dataloaders.{}'.format(appr.experiment))\n",
    "data, taskcla, inputsize = dataloader.get(batch_size=appr.batch_size, val_batch_size=appr.val_batch_size, seed=appr.seed, tasknum=appr.tasknum)\n",
    "\n",
    "u = 0\n",
    "test_loss, test_acc = appr.eval(u+1, data[u]['test loader'])\n",
    "print('>>> Test on task {:2d} - {:15s}: loss={:.3f}, acc={:5.2f}% <<<'.format(u, data[u]['name'], test_loss, 100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "a[:-1]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2043299c89c8cd0b4d1a6f5cf4529bd58e6a4e0fe3181a25e0d328c821cdc5c5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
